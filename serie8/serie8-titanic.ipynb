{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Rev. Juozas Montvila</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Miss. Margaret Edith Graham</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Catherine Helen Johnston</td>\n",
       "      <td>female</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr. Karl Howell Behr</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Patrick Dooley</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>887 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass                                               Name  \\\n",
       "0           0       3                             Mr. Owen Harris Braund   \n",
       "1           1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2           1       3                              Miss. Laina Heikkinen   \n",
       "3           1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4           0       3                            Mr. William Henry Allen   \n",
       "..        ...     ...                                                ...   \n",
       "882         0       2                               Rev. Juozas Montvila   \n",
       "883         1       1                        Miss. Margaret Edith Graham   \n",
       "884         0       3                     Miss. Catherine Helen Johnston   \n",
       "885         1       1                               Mr. Karl Howell Behr   \n",
       "886         0       3                                 Mr. Patrick Dooley   \n",
       "\n",
       "        Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0      male  22.0                        1                        0   7.2500  \n",
       "1    female  38.0                        1                        0  71.2833  \n",
       "2    female  26.0                        0                        0   7.9250  \n",
       "3    female  35.0                        1                        0  53.1000  \n",
       "4      male  35.0                        0                        0   8.0500  \n",
       "..      ...   ...                      ...                      ...      ...  \n",
       "882    male  27.0                        0                        0  13.0000  \n",
       "883  female  19.0                        0                        0  30.0000  \n",
       "884  female   7.0                        1                        2  23.4500  \n",
       "885    male  26.0                        0                        0  30.0000  \n",
       "886    male  32.0                        0                        0   7.7500  \n",
       "\n",
       "[887 rows x 8 columns]"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_FILEPATH = \"data/titanic.csv\"\n",
    "\n",
    "def fetch_data():\n",
    "    \"\"\"Import the data from csv to pd dataframe\"\"\"\n",
    "    relaviteFilepath = os.path.join(os.path.abspath(''), DATA_FILEPATH)\n",
    "    return pd.read_csv(relaviteFilepath)\n",
    "\n",
    "# df stands for dataframe. This is the object that we will manipulate throughouht the notebook\n",
    "titanicdf = fetch_data()\n",
    "titanicdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Take the titanic dataset and using all attributes to predict the class ‘Survived’ \n",
    "\n",
    "* (a)  Choose Three classifiers and evaluate their performance using all attributes\n",
    "* (b)  Define a feature selection method and use it on all the classifiers\n",
    "* (c)  Compare the classifiers and explain the differences observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do some pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>887 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex  Age  Siblings/Spouses Aboard  \\\n",
       "0           0       3    1    1                        1   \n",
       "1           1       1    0    2                        1   \n",
       "2           1       3    0    1                        0   \n",
       "3           1       1    0    2                        1   \n",
       "4           0       3    1    2                        0   \n",
       "..        ...     ...  ...  ...                      ...   \n",
       "882         0       2    1    1                        0   \n",
       "883         1       1    0    1                        0   \n",
       "884         0       3    0    0                        1   \n",
       "885         1       1    1    1                        0   \n",
       "886         0       3    1    2                        0   \n",
       "\n",
       "     Parents/Children Aboard  Fare  \n",
       "0                          0   0.0  \n",
       "1                          0   7.0  \n",
       "2                          0   0.0  \n",
       "3                          0   5.0  \n",
       "4                          0   0.0  \n",
       "..                       ...   ...  \n",
       "882                        0   1.0  \n",
       "883                        0   3.0  \n",
       "884                        2   2.0  \n",
       "885                        0   3.0  \n",
       "886                        0   0.0  \n",
       "\n",
       "[887 rows x 7 columns]"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Drop the names\n",
    "titanicdf.drop(columns=[\"Name\"], axis=1, inplace=True)\n",
    "\n",
    "#The sex also needs to be encoded\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "titanicdf[\"Sex\"] = le.fit_transform(titanicdf[\"Sex\"])\n",
    "\n",
    "#Age, fare -> class\n",
    "titanicdf[\"Fare\"] = titanicdf[\"Fare\"] // 10 #Group by range of 10\n",
    "\n",
    "def group_age(age):\n",
    "    \"\"\"Directly use numbers as we would need to apply label encoder\"\"\"\n",
    "    if age < 18:\n",
    "        return 0 #Child\n",
    "    if age < 30:\n",
    "        return 1 #Yound adult\n",
    "    if age < 60:\n",
    "        return 2 #Adult\n",
    "    return 3 #Senior\n",
    "\n",
    "#group ages\n",
    "titanicdf['Age'] = titanicdf['Age'].apply(group_age)\n",
    "\n",
    "titanicdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare function to get metrics from a model, compare models, and perform feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def get_acc_metrics(Model, xTest: pd.DataFrame, yTest: pd.Series):\n",
    "    ''' Returns accuracy, precision, recall and f1-score of the given model '''\n",
    "    yPred = Model.predict(xTest)\n",
    "\n",
    "    acc = accuracy_score(yTest, yPred)\n",
    "    prec = precision_score(yTest, yPred)\n",
    "    recall = recall_score(yTest, yPred)\n",
    "    f1 = f1_score(yTest, yPred)\n",
    "\n",
    "    return [acc, prec, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "def compare_models(model, x1 : pd.DataFrame, y1 : pd.Series, x2: pd.DataFrame, y2 : pd.Series) -> None:\n",
    "    '''\n",
    "    Train the model with the given [x1,y1] dataset and compare it to the model trained with the [x2,y2] dataset.\n",
    "    '''\n",
    "\n",
    "    metrics = []\n",
    "    passes = [\"original\", \"selected\"]\n",
    "\n",
    "    print(f\"Performing model analysis\")\n",
    "    for X,y in [(x1,y1), (x2, y2)]:\n",
    "        currentModel = clone(model)\n",
    "\n",
    "        #Prepare basis train-test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "        #Fit\n",
    "        currentModel.fit(X_train, y_train)\n",
    "\n",
    "        #Add metrics to list\n",
    "        modelMetrics = get_acc_metrics(currentModel, X_test, y_test)\n",
    "\n",
    "        print(f\"Score on {passes.pop(0)} data : \\n\\tAccuracy = {100*modelMetrics[0]:.2f}%\\n\\tPrecision = {modelMetrics[1]:.2f}\\n\\tRecall = {modelMetrics[2]:.2f}\\n\\tF1 = {modelMetrics[3]:.2f}\")\n",
    "\n",
    "        #Add in list for final comparison\n",
    "        metrics.append(modelMetrics)\n",
    "\n",
    "    diff = []\n",
    "    for i in range(4):\n",
    "        diff.append(metrics[1][i] - metrics[0][i])\n",
    "    print(f\"Gain on model by using the second dataset : \\n\\tAccuracy = {100*diff[0]:.2f}\\n\\tPrecision = {diff[1]:.2f}\\n\\tRecall = {diff[2]:.2f}\\n\\tF1 = {diff[3]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def comparePCA(model, x: pd.DataFrame, y: pd.Series, n: int):\n",
    "    pca = PCA(n_components=n)\n",
    "\n",
    "    xTransformed = pca.fit(x).transform(x)\n",
    "\n",
    "    compare_models(model, x, y, xTransformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def compareRFE(model, x: pd.DataFrame, y: pd.Series, n: int):\n",
    "    rfe = RFE(clone(model), n_features_to_select=n)\n",
    "\n",
    "    rfeFitted = rfe.fit(x, y)\n",
    "    print(f\"Features: {rfeFitted.feature_names_in_}.\\nRanking : {rfeFitted.ranking_}.\\nSelected features : {rfeFitted.feature_names_in_[rfeFitted.get_support(True)]}\")\n",
    "\n",
    "    xTransformed = rfeFitted.transform(x)\n",
    "\n",
    "    compare_models(model, x, y, xTransformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def compareChi2(model, x: pd.DataFrame, y: pd.Series, k: int):\n",
    "    selector = SelectKBest(score_func=chi2, k=k)\n",
    "\n",
    "    selectorFitted = selector.fit(x, y)\n",
    "    print(f\"Features: {selectorFitted.feature_names_in_}.\\nSelected features : {selectorFitted.feature_names_in_[selectorFitted.get_support(True)]}\")\n",
    "\n",
    "    xTransformed = selectorFitted.transform(x)\n",
    "\n",
    "    compare_models(model, x, y, xTransformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset, parameters in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = titanicdf.drop(columns=['Survived'], axis=1, inplace=False), titanicdf[\"Survived\"]\n",
    "features_to_select = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model analysis\n",
      "Score on original data : \n",
      "\tAccuracy = 78.65%\n",
      "\tPrecision = 0.79\n",
      "\tRecall = 0.61\n",
      "\tF1 = 0.69\n",
      "Score on selected data : \n",
      "\tAccuracy = 72.66%\n",
      "\tPrecision = 0.70\n",
      "\tRecall = 0.51\n",
      "\tF1 = 0.59\n",
      "Gain on model by using the second dataset : \n",
      "\tAccuracy = -5.99\n",
      "\tPrecision = -0.09\n",
      "\tRecall = -0.10\n",
      "\tF1 = -0.10\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "comparePCA(tree.DecisionTreeClassifier(), X, y, features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA from 6 to 3 features has overall ~-6% on the accuracy, which starts to be a significant loss. Every metric loses value (~0.1 for each), which means that this dataset (and/or algorithm) seems to not be very receptive of PCA.\\\n",
    "It could also very well be that 6->3 is too much, or that the features don't mix very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Pclass' 'Sex' 'Age' 'Siblings/Spouses Aboard' 'Parents/Children Aboard'\n",
      " 'Fare'].\n",
      "Ranking : [1 1 2 4 3 1].\n",
      "Selected features : ['Pclass' 'Sex' 'Fare']\n",
      "Performing model analysis\n",
      "Score on original data : \n",
      "\tAccuracy = 78.65%\n",
      "\tPrecision = 0.79\n",
      "\tRecall = 0.61\n",
      "\tF1 = 0.69\n",
      "Score on selected data : \n",
      "\tAccuracy = 77.90%\n",
      "\tPrecision = 0.82\n",
      "\tRecall = 0.54\n",
      "\tF1 = 0.65\n",
      "Gain on model by using the second dataset : \n",
      "\tAccuracy = -0.75\n",
      "\tPrecision = 0.04\n",
      "\tRecall = -0.07\n",
      "\tF1 = -0.03\n"
     ]
    }
   ],
   "source": [
    "compareRFE(tree.DecisionTreeClassifier(), X, y, features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCE has fairly good results, it has a ~-1% accuracy but successfully reduces the number of features from 6 to 3.\\\n",
    "The recall and F1-score are lowered at 0.08 and 0.04, but the precision is actually increased, which could be a very good results depending on the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Pclass' 'Sex' 'Age' 'Siblings/Spouses Aboard' 'Parents/Children Aboard'\n",
      " 'Fare'].\n",
      "Selected features : ['Pclass' 'Sex' 'Fare']\n",
      "Performing model analysis\n",
      "Score on original data : \n",
      "\tAccuracy = 78.65%\n",
      "\tPrecision = 0.79\n",
      "\tRecall = 0.61\n",
      "\tF1 = 0.69\n",
      "Score on selected data : \n",
      "\tAccuracy = 77.90%\n",
      "\tPrecision = 0.82\n",
      "\tRecall = 0.54\n",
      "\tF1 = 0.65\n",
      "Gain on model by using the second dataset : \n",
      "\tAccuracy = -0.75\n",
      "\tPrecision = 0.04\n",
      "\tRecall = -0.07\n",
      "\tF1 = -0.03\n"
     ]
    }
   ],
   "source": [
    "compareChi2(tree.DecisionTreeClassifier(), X, y, features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly and even better, the decision tree using chi2 feature selection goes from 6 to 3 features with only -0.75% accuracy. Once again, the precision goes up a little and the recall and f1-score go down at the same level. Loosing 0.75% accuracy for -3 features indicates that these features were not that important for the algorithm, and this is a trade-off that is very benefic (helps reduce complexity for a very low cost and could greatly increase the computation time while using k-folds etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model analysis\n",
      "Score on original data : \n",
      "\tAccuracy = 79.40%\n",
      "\tPrecision = 0.77\n",
      "\tRecall = 0.66\n",
      "\tF1 = 0.71\n",
      "Score on selected data : \n",
      "\tAccuracy = 79.78%\n",
      "\tPrecision = 0.78\n",
      "\tRecall = 0.66\n",
      "\tF1 = 0.72\n",
      "Gain on model by using the second dataset : \n",
      "\tAccuracy = 0.37\n",
      "\tPrecision = 0.01\n",
      "\tRecall = 0.00\n",
      "\tF1 = 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\meachine_learning\\.venv\\lib\\site-packages\\sklearn\\base.py:441: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "XNormalizedArray = preprocessing.MinMaxScaler().fit_transform(X)\n",
    "#Go back to DF to feed in the fct\n",
    "XNormalizedDf = pd.DataFrame(XNormalizedArray, index=X.index, columns=X.columns)\n",
    "\n",
    "\n",
    "comparePCA(KNeighborsClassifier(metric=\"cosine\"), XNormalizedDf, y, features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA achieves very good results ! It has a +0.37% accuracy, with almost the same precision/recall with 3 features compared to 6 features.\\\n",
    "__The normalization of features probably helps the PCA with features merging !__\\\n",
    "The normalization is a very important step of KNN if features are widly different, and it could very well explain why the features can be merged so well. Maybe the normalization step should always be considered for better results when using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot perform RFE on KNN because KNN does not indicate anyting about features (SVM could be used for example) -> go directly to chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Pclass' 'Sex' 'Age' 'Siblings/Spouses Aboard' 'Parents/Children Aboard'\n",
      " 'Fare'].\n",
      "Selected features : ['Pclass' 'Sex' 'Fare']\n",
      "Performing model analysis\n",
      "Score on original data : \n",
      "\tAccuracy = 79.40%\n",
      "\tPrecision = 0.77\n",
      "\tRecall = 0.66\n",
      "\tF1 = 0.71\n",
      "Score on selected data : \n",
      "\tAccuracy = 77.53%\n",
      "\tPrecision = 0.79\n",
      "\tRecall = 0.56\n",
      "\tF1 = 0.66\n",
      "Gain on model by using the second dataset : \n",
      "\tAccuracy = -1.87\n",
      "\tPrecision = 0.02\n",
      "\tRecall = -0.10\n",
      "\tF1 = -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\meachine_learning\\.venv\\lib\\site-packages\\sklearn\\base.py:441: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compareChi2(KNeighborsClassifier(metric=\"cosine\"), XNormalizedDf, y, features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On KNN, the chi2 is a bit worse than the PCA ~-2% accuracy, increases precision marginally, lowers recall sligthly). It is still a very good trade off for a 6->3 feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnbClf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model analysis\n",
      "Score on original data : \n",
      "\tAccuracy = 78.28%\n",
      "\tPrecision = 0.73\n",
      "\tRecall = 0.69\n",
      "\tF1 = 0.71\n",
      "Score on selected data : \n",
      "\tAccuracy = 67.42%\n",
      "\tPrecision = 0.67\n",
      "\tRecall = 0.30\n",
      "\tF1 = 0.42\n",
      "Gain on model by using the second dataset : \n",
      "\tAccuracy = -10.86\n",
      "\tPrecision = -0.06\n",
      "\tRecall = -0.39\n",
      "\tF1 = -0.29\n"
     ]
    }
   ],
   "source": [
    "comparePCA(GaussianNB(), X, y, features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian with PCA lose 10% accuracy from 6 to 3 features, which is a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Pclass' 'Sex' 'Age' 'Siblings/Spouses Aboard' 'Parents/Children Aboard'\n",
      " 'Fare'].\n",
      "Selected features : ['Pclass' 'Sex' 'Fare']\n",
      "Performing model analysis\n",
      "Score on original data : \n",
      "\tAccuracy = 78.28%\n",
      "\tPrecision = 0.73\n",
      "\tRecall = 0.69\n",
      "\tF1 = 0.71\n",
      "Score on selected data : \n",
      "\tAccuracy = 76.40%\n",
      "\tPrecision = 0.69\n",
      "\tRecall = 0.70\n",
      "\tF1 = 0.70\n",
      "Gain on model by using the second dataset : \n",
      "\tAccuracy = -1.87\n",
      "\tPrecision = -0.04\n",
      "\tRecall = 0.01\n",
      "\tF1 = -0.01\n"
     ]
    }
   ],
   "source": [
    "compareChi2(GaussianNB(), X, y, features_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi2 feature selection allow to go from 6 to 3 features with a loss of only ~1.75%, which is still an acceptable trade-off for the simplicity.\\\n",
    "Curiously, while the decision tree had better precision and worst recall with feature selection, the opposite is true with naive bayes: the precision goes down while the recall seems to be a bit better.\\\n",
    "\\\n",
    "The number of features has been set to 3. This is actually an hyper-parameter and the adjustment of this number impacts the performance of each selection algorithm a lot, so these results are to take with a grain of salt."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ed6f5bb5d95a6060489c1e73b807b62943cb606c2473302746af3ad3d902cd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
